# Verosimilitud \label{loglik}

En este capítulo se mostrará como usar \proglang{R} para obtener la función de log-verosimilitud y estimadores por el método de máxima verosimilitud.

## Función de verosimilitud
La función de verosimilitud para un vector de parámetros $\boldsymbol{\Theta}$ dada una muestra aleatoria $\boldsymbol{x}$ con una distribución asumida se define como:

\begin{equation}
L(\boldsymbol{\Theta} | \boldsymbol{x}) = \prod_{i=1}^{n}  f(x_i | \boldsymbol{\Theta}),
(\#eq:lik)
\end{equation}

donde $x_i$ representa uno de los elementos de la muestra aleatoria y $f$ es la función de masa/densidad de la distribución de la cual se obtuvo $\boldsymbol{x}$.

## Función de log-verosimilitud
La función de log-verosimilitud $l$ se define como el logaritmo de la función de verosimilitud $L$, es decir

\begin{equation}
l(\boldsymbol{\Theta} | \boldsymbol{x}) = \log L(\boldsymbol{\Theta} | \boldsymbol{x}) = \sum_{i=1}^{n} \log f(x_i | \boldsymbol{\Theta})
(\#eq:loglik)
\end{equation}

## Método de máxima verosimilitud para estimar parámetros
El método de máxima verosimilitud se usa para estimar los parámetros de una distribución. El objetivo de este método es encontrar los valores de $\boldsymbol{\Theta}$ que maximizan $L$ o $l$ y valores encontrados se representan por $\hat{\boldsymbol{\Theta}}$.

### Ejemplo {-}
Suponga que se tiene el vector `rta` que corresponde a una muestra aleatoria de una distribución binomial con parámetro `size=5` conocido.

```{r}
rta <- c(2, 2, 1, 1, 1, 1, 0, 2, 1, 2,
         1, 0, 1, 2, 1, 0, 0, 2, 2, 1)
```

1) Calcular el valor de log-verosimilitud $l$ si asumimos que $p=0.30$ en la distribución binomial.

Para obtener el valor de $l$ en el punto $p=0.30$ se aplica la definición dada en la expresión \@ref(eq:loglik). Como el problema trata de una binomial se usa entonces la función de masa `dbinom` evaluada en la muestra `rta`, el parámetro `size` como es conocido se reemplaza por el valor de cinco y en el parámetro `prob` se cambia por 0.3. Como interesa la función de log-verosimilitud se debe incluir `log=TRUE`. A continuación el código necesario. 

```{r}
sum(dbinom(x=rta, size=5, prob=0.3, log=TRUE))
```

Por lo tanto $l(\theta)= -24.55$

2) Construir una función llamada `ll` a la cual le ingrese uno o varios valores del parámetro $p$ de la binomial, la función debe entregar el valor de log-verosimilitud en cada uno de los $p$ ingresados.

La función solicitada tiene un cuerpo igual al usado en el numeral anterior. Para asegurar que la función construída pueda recibir un vector de valores de $p$ se vectoriza la función con `Vectorize`, abajo el código usado.

```{r}
ll <- function(prob) sum(dbinom(x=rta, size=5, prob=prob, log=T))
ll <- Vectorize(ll)  # Para vectorizar la función
```

Vamos a probar la función en $p=0.15$ y $p=0.80$ para saber si la función `ll` está vectorizada.

```{r}
ll(prob=0.15)  # Individual para p=0.15
ll(prob=0.80)  # Individual para p=0.80
ll(prob=c(0.15, 0.80))  # Evaluada en un vector 
```

```{block2, type='rmdwarning'}
La mayoría de las funciones en \proglang{R} creadas por los usuarios quedan vectorizadas otras no. La función `ll` no estaba vectorizada y por esa razón se necesitó usa `Vectorize`. No abuse de la función `Vectorize`, sólo úsela cuando sea necesario.
```

3) Dibujar la curva log-verosimilitud $l$.

En la Figura \@ref(fig:loglik1) se presenta la curva solicitada, para su construcción se usa el siguiente código.

```{r loglik1, fig.cap='Función de log-verosimilitud para el ejemplo sobre binomial.', fig.asp=0.9, fig.width=4, echo=TRUE}
curve(ll, lwd=4, col='dodgerblue3',
      xlab='Probabilidad de éxito',
      ylab=expression(paste("l(", theta, ")")))
```

4) Observando la Figura \@ref(fig:loglik1), ¿cuál esl el valor de $p$ que maximiza la función de log-verosimilitud?

Al observar la Figura \@ref(fig:loglik1) se nota que el valor de $p$ que maximiza la función log-verosimilitud está muy cerca de 0.2.

5) ¿Cuál es el valor exacto de $p$ que maximiza la función log-verosimilitud?

En \proglang{R} existe la función `optimize` que sirve para encontrar el valor que __minimiza__ una función uniparamétrica en un intervalo dado, sin embargo, aquí interesa es maximimizar la función de log-verosimilitud, por esa razón se construye la función `minusll` que es el negativo de la función `ll` para así poder usar `optimize`. A continuación el código usado. \index{optimize}

```{r}
minusll <- function(x) -ll(x)

optimize(f=minusll, interval=c(0, 1))
```

Del resultado anterior se observa que cuando $p=0.23$ el valor máximo de log-verosimilitud es 23.32.

### Ejemplo {-}
Suponga que la estatura de una población se puede asumir como una normal $N(170, 25)$. Suponga también que se genera una muestra aleatoria de 50 observaciones de la población con el objetivo de recuperar los valores de la media y varianza poblacionales a partir de la muestra aleatoria.

La muestra se va a generar con la función `rnorm` pero antes se fijará una semilla con la intención de que el lector pueda replicar el ejemplo y obtener la misma muestra aleatoria aquí generada, el código para hacerlo es el siguiente.

```{r}
set.seed(1235)  # La semilla es 1235
y <- rnorm(n=50, mean=170, sd=5)
```

1) construya la función de log-verosimilitud para los parámetros de la normal dada la muestra aleatoria `y`.

```{r}
ll <- function(param) {
  media <- param[1]  # param es el vector de parámetros
  desvi <- param[2] 
  sum(dnorm(x=y, mean=media, sd=desvi, log=TRUE))
}
```

```{block2, type='rmdwarning'}
Siempre que el interés sea encontrar los valores que maximizan una función de log-verosimilitud, los parámetros de la distribución __deben__ ingresar a la función `ll` como un vector. Esto se debe hacer para poder usar las funciones de búsqueda `optim` y `nlminb`.
```

2) Dibuje la función de log-verosimilitud.

En la Figura \@ref(fig:loglik2) se muestra el gráfico de niveles para la superficie de log-verosimilitud. De esta figura se nota claramente que los valores que maximizan la superficie están alrededor de $\mu=170$ y $\sigma=5$. Para ver el código usado en la creación de la Figura \@ref(fig:loglik2) se recomienda consultar la sección sobre `contour` en @hernandez_correa.

```{r loglik2, fig.cap='Gráfico de niveles para la función de log-verosimilitud para el ejemplo sobre normal.', fig.asp=0.6, fig.width=9, echo=FALSE}
ll1 <- function(a, b) sum(dnorm(x=y, mean=a, sd=b, log=TRUE))
ll1 <- Vectorize(ll1)
xx <- seq(from=160, to=180, by=0.5)
yy <- seq(from=3, to=7, by=0.5)
zz <- outer(X=xx, Y=yy, ll1)
filled.contour(x=xx, y=yy, z=zz, nlevels=20,
               xlab=expression(mu), ylab=expression(sigma),
               color = topo.colors)
```

3) Obtenga los valores de $\mu$ y $\sigma$ que maximizan la función de log-verosimilitud.

Para obtener los valores solicitados vamos a usar la función `nlminb` que es un optimizador. A la función `nlminb` se le debe indicar por medio del parámetro `objective` la función que queremos optimizar (minimizar); el parámetro `start` es un vector con los valores iniciales para comenzar la búsqueda de $\mu$ y $\sigma$; los parámetros `lower` y `upper` sirven para delimitar el espacio de búsqueda. A continuación se muestra el código usado para obtener los valores que minimizan a `minusll`, es decir, los valores que maximizan la función de log-verosimilitud. \index{nlminb}

```{r}
minusll <- function(x) -ll(x)
nlminb(objective=minusll, start=c(163, 3.4),
       lower=c(160, 3), upper=c(180, 7))
```

De la salida anterior podemos observar que los valores óptimos de $\mu$ y $\sigma$ son  170.338 y 5.424 respectivamente, esto coincide con lo observado en la \@ref(fig:loglik2) y con los valores reales de simulación de la muestra. Esto indica que el procedimiento de estimación de parámetros por máxima verosimilitud entrega valores insesgados de los parámetros a estimar.

Un resultado interesante de la salida anterior es que se reporta el valor mínimo que alcanza la función `minusll`, este valor fue de  155.5, por lo tanto, se puede afirmar que el valor máximo de log-verosimilitud es -155.5.

Otros resultados importantes de la salida anterior son el valor de `convergence=0` que indica que la búsqueda fue exitosa; `iterations=13` indica que se realizaron 13 pasos desde el punto inicial `start` hasta las coordenadas de optimización.


```{block2, type='rmdnote'}
En \proglang{R} se tienen dos funciones básicas para optimizar funciones, es decir, para encontrar los valores que minimizan una función dada. Esas dos funciones son `nliminb` y `optim`. Para optimizar en una sola dimensión se usa la función `optimize`.
```

4) ¿Hay alguna función para obtener directamente el valor que maximiza la función log-verosimilitud?

La respuesta es si. Si la distribución estudiada es una de las distribuciones básicas se puede usar la función `fitdistr` del paquete básico **MASS**. La función `fitdistr` requiere de los datos que se ingresan por medio del parámetro `x`, y de la distribución de los datos que se ingrea por medio del parámetro `densfun`. La función `fitdistr` admite 15 distribuciones diferentes para hacer la búsqueda de los parámetros que caracterizan una distribución, se sugiere consultar la ayuda de la función `fitdistr` escribiendo en la consola `help(fitdistr)`. A continuación el código usado. \index{fitdistr}

```{r, message=FALSE}
require(MASS) # El paquete ya está instalado, solo se debe cargar
res <- fitdistr(x=y, densfun='normal')
res
```

El objeto `res` contiene los resultados de usar `fitdistr`. En la primer línea están los valores de los parámetros que maximizan la función de log-verosimilitud, en la parte de abajo, dentro de paréntesis, están los errores estándar o desviaciones de éstos estimadores.

Al objeto `res` es de la clase _fitdistr_ y por lo tanto se le puede aplicar la función genérica `logLik` para obtener el valor de la log-verosimilitud. Se sugiere consultar la ayuda de la función `logLik` escribiendo en la consola `help(logLik)`. A continuación el código para usar `logLik` sobre el objeto `res`.

```{r}
logLik(res)
```

De esta última salida se observa que el valor coincide con el obtenido cuando se usó `nlminb`.

### Ejemplo {-}
Generar $n=100$ observaciones de una gamma con parámetro de forma igual a 2 y parámetro de tasa igual a 0.5 y luego responder las preguntas.

Para generar la muestra aleatoria `ma` solicitada se fijó la semilla con el objetivo de que el lector pueda obtener los mismos resultados de este ejemplo.

```{r}
n <- 100
set.seed(12345)
ma <- rgamma(n=n, shape=2, rate=0.5)
```

1) Asumiendo que la muestra aleatoria proviene de una normal (lo cual es incorrecto) estime los parámetros de la distribución

```{r}
fit1 <- fitdistr(x=ma, densfun='normal')
fit1
```

2) Asumiendo que la muestra aleatoria proviene de una gamma estime los parámetros de la distribución.

```{r, warning=FALSE}
fit2 <- fitdistr(x=ma, densfun='gamma')
fit2
```

En la salida anterior están los valores estimados de los parámetros de la distribución por el método de máxima verosimilitud, observe la cercanía de éstos con los verdaderos valores de 2 y 0.5 para forma y tasa respectivamente.

3) Dibuje dos qqplot, uno asumiendo distribución gamma y el otro distribución gamma. ¿Cuál distribución se ajusta mejor a los datos simulados?

Para dibujar el qqplot se usa la función genérica `qqplot`, recomendamos consultar @hernandez_correa para los detalles de cómo usar esta función. Al usar `qqplot` para obtener el qqplot normal y gamma es necesario indicar los valores $\hat{\boldsymbol{\Theta}}$ obtenidos en el numeral anterior, por eso es que en el código mostrado a continuación aparece `mean=4.3083, sd=2.8085` en el qqplot normal y `shape=2.23978, rate=0.51988` en el qqplot gamma.

```{r normgamma, fig.cap='Gráfico cuantil cuantil normal y gamma para la muestra simulada.', fig.asp=0.6, fig.width=9}
par(mfrow=c(1, 2))

qqplot(y=ma, pch=19,
       x=qnorm(ppoints(n), mean=4.3083, sd=2.8085),
       main='Normal Q-Q Plot',
       xlab='Theoretical Quantiles',
       ylab='Sample Quantiles')

qqplot(y=ma, pch=19,
       x=qgamma(ppoints(n), shape=2.23978, rate=0.51988),
       main='Gamma Q-Q Plot',
       xlab='Theoretical Quantiles',
       ylab='Sample Quantiles')
```

En la Figura \@ref(fig:normgamma) se muestran los qqplot solicitados. Se observa claramente que al asumir normalidad (lo cual es incorrecto), los puntos del qqplot no están alineados, mientras que al asumir distribución gamma (lo cual es correcto), los puntos si están alineados. De esta figura se concluye que la muestra `ma` puede provenir de una $Gamma(2.23978, 0.51988)$.

```{block2, type='rmdtip'}
Para obtener el gráfico cuantil cuantil bajo normalidad se puede usar directamente la función `qqnorm`, consultar @hernandez_correa para mayores detalles.
```

```{block2, type='rmdwarning'}
En este ejemplo se eligió la mejor distribución entre dos candidatas usando una herramienta gráfica, lo que se recomienda usar algún método menos subjetivo (cuantitativo) para tomar decisiones. 
```

4) Para comparar modelos se puede utilizar el _Akaike information criterion_ ($AIC$) propuesto por @Akaike74 que sirve para medir la calidad relativa de los modelos estadísticos, la expresión para calcular el indicador es $AIC=-2 \, \hat{l}+2 \, df$, donde $\hat{l}$ corresponde al valor de $\log$-verosimilitud y $df$ corresponde al número de parámetros estimados del modelo. Siempre el modelo elegido es aquel modelo con el menor valor de $AIC$. Calcular el $AIC$ para los modelos asumidos normal y gamma.

```{r}
-2 * logLik(fit1) + 2 * 2  # AIC para modelo normal
-2 * logLik(fit2) + 2 * 2  # AIC para modelo gamma
```

De los resultados anteriores se concluye que entre los dos modelos, el mejor es el gamma porque su $AIC=466$ es el menor de toos los $AIC$.

```{block2, type='rmdnote'}
Modelos anidados pueden ser comparados por medio del _global deviance_ ($GD$) dado por $GD=-2 \, \hat{l}$ y modelos no anidados por medio del _Generalized Akaike information criterion_ ($GAIC$) propuesto por @Akaike83 y dado por $GAIC=-2 \, \hat{l} + \# \, df$ siendo $\#$ el valor de penalidad por cada parámetro adicional en el modelo; cuando $\# = 2$, el $GAIC$ coincide con el $AIC$ y el _Schwarz Bayesian criterion_ ($SBC$) propuesto por @Schwarz se dá cuando el valor de penalidad es $\# = \log(n)$ donde $n$ es el número de observaciones del modelo; siempre el modelo elegido es aquel modelo con el menor valor de cualquiera de los criterios de información anteriores.
```

### Ejemplo {-}
¿Será posible explorar entre muchas distribuciones estadísticas aquella que mejor explica una variable de interés?

La respuesta es si, se puede usar la función `fitDist` del paquete **gamlss** \index{gamlss}. Esta función tiene la siguiente estructura:

```{r, eval=FALSE}
fitDist(y, k = 2, 
      type = c("realAll", "realline", "realplus",
               "real0to1", "counts", "binom"))
```

El parámetro `y` sirve para ingresar el vector con la información; `k=2` es la penalización por cada parámetro estimado para calcular el $GAIC$, por defecto es 2; y el parámetro `type` sirve para indicar el tipo de distribución, los posibles valores son:

- `realAll`: para hacer la búsqueda en todas las distribuciones disponibles en **gamlss**.
- `realline`: para variables en $\Re$.
- `realplus`: para variables en $\Re^+$.
- `real0to1`: para variables en el intervalo $(0, 1)$.
- `counts`: para variables de conteo.
- `binom`: para variables de tipo binomial.

1) Usar la función `fitDist` para elegir la distribución que mejor se ajusta a los datos simulados en el ejemplo anterior, usar una penalizacion de $k=2$ para calcular el $AIC$.

Se usa la función `fitDist` con `type='realplus'` porque se sabe que la muestra aleatoria tiene valores sólo en $\Re^+$, los resultados de almacenan en el objeto `modelos`. Para obtener la lista de los mejores modelos con su respectivo $BAIC$ se escribe en la consola `modelos$fits`. Abajo el código usado.

```{r, message=FALSE, warning=FALSE}
require(gamlss)
modelos <- fitDist(y=ma, type='realplus', k=2)
modelos$fits
```

De la lista anterior se observa que la función gamma está en el primer lugar con un $BAIC=466$, valor que coincide con el $AIC$ obtenido en el ejercicio anterior. Esto significa que como la gamma tiene el menor $BAIC$, es la distribución que mejor se ajusta a los datos de la muestra, esto coincide con la realidad, ya que la muestra fue generada de una distribución gamma.

¿Por queé $AIC$ y $BAIC$ coinciden en este ejemplo?

2) ¿Cuáles son los valores estimados de los parámetros para la distribución gamma identificada en el paso anterior?

En **gamlss** los parámetros de las distribuciones se nombran de una forma especial, el parámetro de posición se denomina $\mu$, el de dispersión se denomina $\sigma$ y los de simetría y curtosis con $\nu$ y $\tau$. La distribución gamma sólo tiene 2 parámetros y para obtener sus valores estimados se usa el siguiente código.

```{r}
modelos$mu
modelos$sigma
```

Si comparamos los anteriores resultados, $\hat{\mu}= 4.308$ y $\hat{\sigma}=0.6682$, con los encontrados al usar la función `fitdist`, $\widehat{shape}=2.23978$ y $\widehat{rate}=0.51988$, vemos que no coinciden, esto se debe a que la parametrización de la gamma usada por **gamlss** es diferente a la parametrización usada por la base de \proglang{R}. Lo anterior no es motivo de preocupación, lo que se recomienda es que el usuario elija una de las parametrizaciones y trabaje con ella, no debe mezclarlas.

3) Dibujar el histograma para la muestra aleatoria y agregar la densidad de la distribución gamma identificada como la distribución que mejor explica el comportamiento de la variable.

Para hacer lo solicitado se usa la función `histDist` del paquete **gamlss**, sólo es necesario ingresar los datos y la familia o distribución de la densidad requerida. Abajo el código usado.

```{r histDist1, fig.cap='Histograma para la muestra simulada con la densidad de una Gamma(mu=4.308, sigma=0.6682).', fig.asp=0.6, fig.width=9}
histDist(y=ma, family=GA, main='', xlab='x', ylab='Densidad',
         line.col='blue', line.wd=4)
```

En la Figura \@ref(fig:histDist1) se presenta el histograma para muestra aleatoria y la densidad de la gamma que mejor explica estos datos. Se observa claramente que la curva de densidad azul acompaña el patrón de los datos.

## Método de máxima verosimilitud para estimar parámetros en modelos de regresión
En la sección anterior se estudió cómo estimar los parámetros de una distribución, en este capítulo se aprenderá a estimar los parámetros de un modelo de regresión general.

### Ejemplo {-}
Considere el modelo de regresión mostrado abajo. Simule 1000 observaciones del modelo y use la función `optim` para estimar los parámetros del modelo.

\begin{align*}
y_i &\sim N(\mu_i, \sigma^2), \\
\mu_i &= -2 + 3 x_1, \\
\sigma &= 5, \\
x_1 &\sim U(-5, 6).
\end{align*}

El código mostrado a continuación permite simular un conjunto de valores con la estructura anteior.

```{r}
n <- 1000
x1 <- runif(n=n, -5, 6)
y <- rnorm(n=n, mean=-2 + 3 * x1, sd=5)
```


El vector de parámetros del modelo anterior es $\boldsymbol{\Theta}=(\beta_0, \beta_1, \sigma)^\top=(-2, 3, 5)^\top$, el primer elemento corresponde al intercepto, el segundo a la pendiente y el último a la desviación.

A continuación se define la función de __menos__ log-verosimilitud para el modelo anterior. A pesar de que nos interesa maximizar la función de log-verosimilitud hemos creado su negativo, esto porque la mayoría de las funciones de optimización minimizan y no maximizan; maximizar $f(x)$ es equivalente a minimizar $-f(x)$.

```{r}
minusll <- function(theta, y, x1) {
  media <- theta[1] + theta[2] * x1  # Se define la media
  desvi <- theta[3]                  # Se define la desviación.
  - sum(dnorm(x=y, mean=media, sd=desvi, log=TRUE))
}
```

Ahora vamos a usar la función `optim` para encontrar los valores que maximizan la función de log-verosimilitud, el código para hacer eso se muestra a continuación. En el parámetro `par` se coloca un vector de posibles valores de $\boldsymbol{\Theta}$ para iniciar la búsqueda, en `fn` se coloca la función de interés, en `lower` y `upper` se colocan vectores que indican los límites de búsqueda de cada parámetro, los $\beta_k$ pueden variar entre $-\infty$ y $\infty$ mientras que el parámetro $\sigma$ toma valores en el intervalo $(0, \infty)$. Como la función `minusll` tiene argumentos adicionales `y` e `x1`, estos pasan a la función `optim` al final como se muestra en el código.

```{r}
res1 <- optim(par=c(0, 0, 1), fn=minusll,
              method='L-BFGS-B',
              lower=c(-Inf, -Inf, 0),
              upper=c(Inf, Inf, Inf), y=y, x1=x1)
```

En el objeto `res1` está el resultado de la optimización, para explorar los resultados usamos

```{r}
res1
```

De la salida anterior se observa que el vector de parámetros estimado es `r res1$par` y que el valor de la máxima log-verosimilitud fue de `r res1$value`. Vemos entonces que el vector estimado está muy cerca del verdadero $\boldsymbol{\Theta}=(\beta_0, \beta_1, \sigma)^\top=(-2, 3, 5)^\top$.

En algunas ocasiones es mejor hacer la búsqueda de los parámetros en el intervalo $(-\infty, \infty)$ que en una región limitada como por ejemplo $(0, \infty)$ o $(-1, 1)$, ya que las funciones de búsqueda podrían tener problemas en los límites numéricos. 
Una estrategia usual en este tipo de casos es aplicar una transformación apropiada al parámetro que tiene el dominio limitado. En el presente ejemplo $\sigma$ sólo puede tomar valores mayores que cero y una transformación de tipo $\log$ podría ser muy útil ya que $\log$ relaciona los reales positivos con todos los reales. La transformación para este problema sería $\log(\sigma)=\beta_3$ o escrita de forma inversa $\sigma=\exp(\beta_3)$. El nuevo parámetro $\beta_3$ puede variar en $(-\infty, \infty)$ pero al ser transformado por la función exponencial este se volvería un valor apropiado para $\sigma$. Para implementar esta variación lo único que se debe hacer es modificar la línea 3 de la función `minusll` como se muestra a continuación:

```{r}
minusll <- function(theta, y, x1) {
  media <- theta[1] + theta[2] * x1  
  desvi <- exp(theta[3])  # <<<<<---- El cambio fue aquí
  - sum(dnorm(x=y, mean=media, sd=desvi, log=TRUE))
}
```

Para hacer la búsqueda se procede de forma similar, abajo el código necesario.

```{r}
res2 <- optim(par=c(0, 0, 0), fn=minusll, y=y, x1=x1)
```

Para obtener los valores ajustados y almacenados en el objeto `res2` usamos el siguiente código.

```{r}
c(res2$par[1:2], exp(res2$par[3]))
```

De la salida anterior se observa que el vector estimado está muy cerca del verdadero $\boldsymbol{\Theta}=(\beta_0, \beta_1, \sigma)^\top=(-2, 3, 5)^\top$.

## EJERCICIOS {-}

1) Al inicio del Capítulo \@ref(central) se presentó la base de datos sobre medidas del cuerpo, consulte la explicación sobre la base de datos y responda lo siguiente.
  + Si se asume que la `edad` tiene distribución normal, ¿cuáles son los estimadores de máxima verosimilitud para $\mu$ y $\sigma$?
  + Como el histograma para la edad muestra un sesgo a la derecha se podría pensar que la distribución gamma sería una buena candidata para explicar las edades observadas. Asumiendo una distribución gamma, ¿cuáles son los estimadores de máxima verosimilitud para los parámetros?
  + ¿Cuál de los dos modelos es más apropiado para explicar la variable de interés? Calcule el $AIC$ para decidir.

2) En la sección \@ref(crabs) se presentó la base de datos sobre cangrejos hembra, consulte la explicación sobre la base de datos y responda lo siguiente.
  + Suponga que el número de satélites sobre cada hembra es una variable que se distribuye Poisson. Construya en \proglang{R} la función de log-verosimilitud $l$, dibuje la función $l$ y encuentre el estimador de máxima verosimilitud de $\lambda$.
  + Repita el ejercicio anterior asumiendo que el número de satélites se distribuye binomial negativo.
  + ¿Cuál de los dos modelos es más apropiado para explicar la variable de interés? Calcule el $AIC$ para decidir.
  
3) Al inicio del Capítulo \@ref(varia) se presentó la base de datos sobre apartamentos usados en Medellín, consulte la explicación sobre la base de datos y responda lo siguiente.
  + Dibuje una densidad para la variable área del apartamento.
  + Describa lo encontrado en esa densidad.
  + ¿Qué distribuciones de 2 parámetros podrían explicar el comportamiento del área de los apartamentos? Mencione al menos 3.
  + Para cada una de las distribuciones anteriores dibuje un gráfico de contornos o calor para la función de log-verosimilitud y estime los parámetros de la distribución elegida.
  + ¿Cuál de los dos modelos es más apropiado para explicar la variable de interés? Calcule el $AIC$ para decidir.
  
4) Simule el siguiente modelo de regresión.

\begin{align*}
y_i &\sim Gamma(shape_i, scale_i), \\
shape_i &= -2 + 3 x_1, \\
scale_i &= 5 - 2 x_2, \\
x_1 &\sim U(-5, 6), \\
x_2 &\sim Poisson(\lambda=3)-
\end{align*}


